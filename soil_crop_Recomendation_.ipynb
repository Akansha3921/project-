{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCH3ETDLYHRXC4PSKAsJMx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akansha3921/project-/blob/main/soil_crop_Recomendation_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pickle\n",
        "import tkinter as tk\n",
        "from tkinter import messagebox\n",
        "import os\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "def load_data():\n",
        "    dataset_path = \"soil_data.csv\"\n",
        "    data = pd.read_csv(dataset_path)\n",
        "    return data\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "def preprocess_data(data):\n",
        "    X = data[['N', 'P', 'K', 'temperature', 'humidity', 'ph']]\n",
        "    y = data['label']\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train the model\n",
        "def train_model(X_train, y_train):\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "# Step 4: Evaluate the model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 5: Save the model\n",
        "def save_model(model, filename=\"crop_recommendation_model.pkl\"):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(model, file)\n",
        "\n",
        "# Step 6: Load the model\n",
        "def load_model(filename=\"crop_recommendation_model.pkl\"):\n",
        "    with open(filename, 'rb') as file:\n",
        "        return pickle.load(file)\n",
        "\n",
        "# Step 7: Recommend crop\n",
        "def recommend_crop_cli():\n",
        "    try:\n",
        "        N = float(input(\"Enter Nitrogen (N): \"))\n",
        "        P = float(input(\"Enter Phosphorus (P): \"))\n",
        "        K = float(input(\"Enter Potassium (K): \"))\n",
        "        temperature = float(input(\"Enter Temperature: \"))\n",
        "        humidity = float(input(\"Enter Humidity: \"))\n",
        "        pH = float(input(\"Enter pH: \"))\n",
        "\n",
        "        model = load_model()\n",
        "        input_data = [[N, P, K, temperature, humidity, ph]]\n",
        "        prediction = model.predict(input_data)\n",
        "\n",
        "        print(f\"Recommended Crop: {prediction[0]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Step 8: Create a GUI for crop recommendation\n",
        "def create_gui():\n",
        "    global entry_N, entry_P, entry_K, entry_temperature, entry_humidity, entry_pH\n",
        "\n",
        "    root = tk.Tk()\n",
        "    root.title(\"Crop Recommendation System\")\n",
        "\n",
        "    tk.Label(root, text=\"Nitrogen (N):\").grid(row=0, column=0, padx=10, pady=5)\n",
        "    entry_N = tk.Entry(root)\n",
        "    entry_N.grid(row=0, column=1, padx=10, pady=5)\n",
        "\n",
        "    tk.Label(root, text=\"Phosphorus (P):\").grid(row=1, column=0, padx=10, pady=5)\n",
        "    entry_P = tk.Entry(root)\n",
        "    entry_P.grid(row=1, column=1, padx=10, pady=5)\n",
        "\n",
        "    tk.Label(root, text=\"Potassium (K):\").grid(row=2, column=0, padx=10, pady=5)\n",
        "    entry_K = tk.Entry(root)\n",
        "    entry_K.grid(row=2, column=1, padx=10, pady=5)\n",
        "\n",
        "    tk.Label(root, text=\"Temperature:\").grid(row=3, column=0, padx=10, pady=5)\n",
        "    entry_temperature = tk.Entry(root)\n",
        "    entry_temperature.grid(row=3, column=1, padx=10, pady=5)\n",
        "\n",
        "    tk.Label(root, text=\"Humidity:\").grid(row=4, column=0, padx=10, pady=5)\n",
        "    entry_humidity = tk.Entry(root)\n",
        "    entry_humidity.grid(row=4, column=1, padx=10, pady=5)\n",
        "\n",
        "    tk.Label(root, text=\"pH:\").grid(row=5, column=0, padx=10, pady=5)\n",
        "    entry_ph = tk.Entry(root)\n",
        "    entry_ph.grid(row=5, column=1, padx=10, pady=5)\n",
        "\n",
        "    tk.Button(root, text=\"Recommend Crop\", command=recommend_crop).grid(row=6, column=0, columnspan=2, pady=10)\n",
        "\n",
        "    root.mainloop()\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    data = load_data()\n",
        "    X_train, X_test, y_train, y_test = preprocess_data(data)\n",
        "    model = train_model(X_train, y_train)\n",
        "    evaluate_model(model, X_test, y_test)\n",
        "    save_model(model)\n",
        "    print(\"Model training complete and saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    if os.environ.get(\"DISPLAY\"):\n",
        "        create_gui()\n",
        "    else:\n",
        "        print(\"No display found. Switching to command-line interface.\")\n",
        "        recommend_crop_cli()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQkvPX_N1gDF",
        "outputId": "e6dc6172-8d67-441d-997b-1e31fde3b997"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9727272727272728\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       1.00      0.90      0.95        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       1.00      1.00      1.00        17\n",
            "      cotton       1.00      1.00      1.00        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.88      0.96      0.92        23\n",
            " kidneybeans       1.00      1.00      1.00        20\n",
            "      lentil       0.69      1.00      0.81        11\n",
            "       maize       1.00      1.00      1.00        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       0.95      0.88      0.91        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       0.91      0.87      0.89        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.94      0.84      0.89        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.97       440\n",
            "   macro avg       0.97      0.97      0.97       440\n",
            "weighted avg       0.98      0.97      0.97       440\n",
            "\n",
            "Model training complete and saved.\n",
            "No display found. Switching to command-line interface.\n",
            "Enter Nitrogen (N): 91\n",
            "Enter Phosphorus (P): 82\n",
            "Enter Potassium (K): 75\n",
            "Enter Temperature: 26\n",
            "Enter Humidity: 83\n",
            "Enter pH: 6\n",
            "An error occurred: name 'ph' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Check if the dataset file exists\n",
        "dataset_path = \"soil_data.csv\"  # Replace with the actual path\n",
        "if not os.path.exists(dataset_path):\n",
        "    raise FileNotFoundError(f\"The dataset file '{dataset_path}' was not found. Please make sure the file exists in the specified path.\")\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Feature columns and target column\n",
        "features = ['N', 'P', 'K', 'temperature', 'humidity', 'ph']\n",
        "target = 'label'  # The label column contains the crop names\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier model\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the trained model to a file\n",
        "with open(\"decision_tree_model.pkl\", \"wb\") as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "print(\"Model saved as 'decision_tree_model.pkl'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlKT6Leo2rvn",
        "outputId": "4d3b4749-7992-4086-a74b-c7fe64f64ff3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 96.82%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.95      0.95      0.95        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       1.00      1.00      1.00        17\n",
            "      cotton       1.00      1.00      1.00        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.88      0.91      0.89        23\n",
            " kidneybeans       1.00      1.00      1.00        20\n",
            "      lentil       0.79      1.00      0.88        11\n",
            "       maize       1.00      1.00      1.00        21\n",
            "       mango       1.00      0.89      0.94        19\n",
            "   mothbeans       1.00      0.83      0.91        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       0.81      0.91      0.86        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.89      0.84      0.86        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.97       440\n",
            "   macro avg       0.97      0.97      0.97       440\n",
            "weighted avg       0.97      0.97      0.97       440\n",
            "\n",
            "Model saved as 'decision_tree_model.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Define the dataset path\n",
        "dataset_path = \"soil_data.csv\"\n",
        "\n",
        "# Example dataset creation if file does not exist\n",
        "if not os.path.exists(dataset_path):\n",
        "    print(f\"The dataset file '{dataset_path}' was not found. Creating an example dataset...\")\n",
        "    example_data = {\n",
        "        'N': [90, 85, 78, 92],\n",
        "        'P': [42, 40, 35, 45],\n",
        "        'K': [60, 55, 50, 65],\n",
        "        'temperature': [30, 32, 28, 31],\n",
        "        'humidity': [80, 85, 75, 78],\n",
        "        'ph': [6.5, 6.8, 7.0, 6.2],\n",
        "        'label': ['Wheat', 'Rice', 'Maize', 'Barley']\n",
        "    }\n",
        "    pd.DataFrame(example_data).to_csv(dataset_path, index=False)\n",
        "    print(f\"Example dataset saved as '{dataset_path}'. Please replace it with your actual dataset if needed.\")\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Feature columns and target column\n",
        "features = ['N', 'P', 'K', 'temperature', 'humidity', 'ph']\n",
        "target = 'label'  # The label column contains the crop names\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier model\n",
        "model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the trained model to a file\n",
        "with open(\"random_forest_model.pkl\", \"wb\") as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "print(\"Model saved as 'random_forest_model.pkl'\")\n",
        "\n",
        "# Function to predict crop based on user input\n",
        "def predict_crop(N, P, K, temperature, humidity, ph):\n",
        "    with open(\"random_forest_model.pkl\", \"rb\") as file:\n",
        "        loaded_model = pickle.load(file)\n",
        "    input_features = pd.DataFrame([[N, P, K, temperature, humidity, ph]], columns=features)\n",
        "    prediction = loaded_model.predict(input_features)\n",
        "    return prediction[0]\n",
        "\n",
        "# Example usage of the function\n",
        "if __name__ == \"__main__\":\n",
        "    # Example values for N, P, K, temperature, humidity, and ph\n",
        "    N = 90\n",
        "    P = 40\n",
        "    K = 60\n",
        "    temperature = 30\n",
        "    humidity = 80\n",
        "    ph = 6.5\n",
        "\n",
        "    recommended_crop = predict_crop(N, P, K, temperature, humidity, ph)\n",
        "    print(f\"The recommended crop for the given values is: {recommended_crop}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKrN3GuA4DJf",
        "outputId": "635398d5-d3a2-4f9b-9ce0-436ecfdd560b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 97.27%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       1.00      0.90      0.95        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       1.00      1.00      1.00        17\n",
            "      cotton       1.00      1.00      1.00        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.88      0.96      0.92        23\n",
            " kidneybeans       1.00      1.00      1.00        20\n",
            "      lentil       0.69      1.00      0.81        11\n",
            "       maize       1.00      1.00      1.00        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       0.95      0.88      0.91        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       0.91      0.87      0.89        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.94      0.84      0.89        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.97       440\n",
            "   macro avg       0.97      0.97      0.97       440\n",
            "weighted avg       0.98      0.97      0.97       440\n",
            "\n",
            "Model saved as 'random_forest_model.pkl'\n",
            "The recommended crop for the given values is: jute\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"soil_data.csv\")\n",
        "\n",
        "# Check the columns in the dataset to ensure they match\n",
        "print(\"Columns in the dataset:\", data.columns)\n",
        "\n",
        "# Rename the pH column to lowercase 'ph'\n",
        "data = data.rename(columns={\"pH\": \"ph\"})\n",
        "\n",
        "# Check if the 'humidity' column is available and print the first few rows for confirmation\n",
        "print(data.head())\n",
        "\n",
        "# Feature selection - assuming N, P, K, ph, temperature, humidity are the input features\n",
        "X = data[['N', 'P', 'K', 'ph', 'temperature', 'humidity']]\n",
        "\n",
        "# Target variable - using 'label' as the column indicating the suitable crop\n",
        "y = data['label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling - Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-REfYXT7OLW",
        "outputId": "9e144350-e030-49f4-cb2a-b84fd9d59f02"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the dataset: Index(['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'label'], dtype='object')\n",
            "    N   P   K  temperature   humidity        ph label\n",
            "0  90  42  43    20.879744  82.002744  6.502985  rice\n",
            "1  85  58  41    21.770462  80.319644  7.038096  rice\n",
            "2  60  55  44    23.004459  82.320763  7.840207  rice\n",
            "3  74  35  40    26.491096  80.158363  6.980401  rice\n",
            "4  78  42  42    20.130175  81.604873  7.628473  rice\n",
            "Accuracy: 90.91%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.85      0.85      0.85        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       0.94      1.00      0.97        17\n",
            "      cotton       0.94      1.00      0.97        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.65      0.65      0.65        23\n",
            " kidneybeans       0.86      0.95      0.90        20\n",
            "      lentil       0.62      0.91      0.74        11\n",
            "       maize       0.95      0.95      0.95        21\n",
            "       mango       0.86      1.00      0.93        19\n",
            "   mothbeans       0.94      0.62      0.75        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       0.96      1.00      0.98        23\n",
            "  pigeonpeas       0.70      0.61      0.65        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.65      0.58      0.61        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.91       440\n",
            "   macro avg       0.91      0.91      0.91       440\n",
            "weighted avg       0.91      0.91      0.91       440\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"soil_data.csv\")\n",
        "\n",
        "# Check the columns in the dataset to ensure they match\n",
        "print(\"Columns in the dataset:\", data.columns)\n",
        "\n",
        "# Rename the pH column to lowercase 'ph'\n",
        "data = data.rename(columns={\"pH\": \"ph\"})\n",
        "\n",
        "# Feature selection - assuming N, P, K, ph, temperature, humidity are the input features\n",
        "X = data[['N', 'P', 'K', 'ph', 'temperature', 'humidity']]\n",
        "\n",
        "# Target variable - using 'label' as the column indicating the suitable crop\n",
        "y = data['label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling - Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the Support Vector Machine model\n",
        "model = SVC(kernel='linear')  # You can use 'linear', 'rbf', 'poly', etc.\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS5_W39T9mfZ",
        "outputId": "03b56838-6c27-4bb2-85a2-b03b83ae45a5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the dataset: Index(['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'label'], dtype='object')\n",
            "Accuracy: 92.50%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.84      0.80      0.82        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       0.94      1.00      0.97        17\n",
            "      cotton       0.94      1.00      0.97        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.71      0.74      0.72        23\n",
            " kidneybeans       0.95      1.00      0.98        20\n",
            "      lentil       0.56      0.91      0.69        11\n",
            "       maize       1.00      0.95      0.98        21\n",
            "       mango       0.95      1.00      0.97        19\n",
            "   mothbeans       0.89      0.71      0.79        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       0.80      0.70      0.74        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.71      0.63      0.67        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.93       440\n",
            "   macro avg       0.92      0.93      0.92       440\n",
            "weighted avg       0.93      0.93      0.92       440\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"soil_data.csv\")\n",
        "\n",
        "# Check the columns in the dataset to ensure they match\n",
        "print(\"Columns in the dataset:\", data.columns)\n",
        "\n",
        "# Rename the pH column to lowercase 'ph'\n",
        "data = data.rename(columns={\"pH\": \"ph\"})\n",
        "\n",
        "# Feature selection - assuming N, P, K, ph, temperature, humidity are the input features\n",
        "X = data[['N', 'P', 'K', 'ph', 'temperature', 'humidity']]\n",
        "\n",
        "# Target variable - using 'label' as the column indicating the suitable crop\n",
        "y = data['label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling - Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the K-Nearest Neighbors model\n",
        "model = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCscYUSX9sxu",
        "outputId": "5cc8c846-bed1-43f3-f9fa-ba4e4025ae13"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the dataset: Index(['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'label'], dtype='object')\n",
            "Accuracy: 90.00%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       0.96      1.00      0.98        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.78      0.90      0.84        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       0.96      1.00      0.98        27\n",
            "      coffee       0.89      1.00      0.94        17\n",
            "      cotton       0.89      1.00      0.94        17\n",
            "      grapes       1.00      0.93      0.96        14\n",
            "        jute       0.67      0.78      0.72        23\n",
            " kidneybeans       0.91      1.00      0.95        20\n",
            "      lentil       0.47      0.73      0.57        11\n",
            "       maize       1.00      0.86      0.92        21\n",
            "       mango       0.86      1.00      0.93        19\n",
            "   mothbeans       0.94      0.67      0.78        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       0.89      1.00      0.94        17\n",
            "      orange       1.00      0.93      0.96        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       0.81      0.57      0.67        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.71      0.53      0.61        19\n",
            "  watermelon       1.00      0.89      0.94        19\n",
            "\n",
            "    accuracy                           0.90       440\n",
            "   macro avg       0.90      0.90      0.89       440\n",
            "weighted avg       0.91      0.90      0.90       440\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"soil_data.csv\")\n",
        "\n",
        "# Check the columns in the dataset to ensure they match\n",
        "print(\"Columns in the dataset:\", data.columns)\n",
        "\n",
        "# Feature selection - assuming N, P, K, ph, temperature, humidity are the input features\n",
        "X = data[['N', 'P', 'K', 'ph', 'temperature', 'humidity']]\n",
        "\n",
        "# Target variable - using 'label' as the column indicating the suitable crop\n",
        "y = data['label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling - Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the K-Nearest Neighbors model\n",
        "model = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2On5I2r-Aq_",
        "outputId": "7b9a96e7-d192-4006-bfa0-e58f42cbcbfc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the dataset: Index(['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'label'], dtype='object')\n",
            "Accuracy: 90.00%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       0.96      1.00      0.98        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.78      0.90      0.84        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       0.96      1.00      0.98        27\n",
            "      coffee       0.89      1.00      0.94        17\n",
            "      cotton       0.89      1.00      0.94        17\n",
            "      grapes       1.00      0.93      0.96        14\n",
            "        jute       0.67      0.78      0.72        23\n",
            " kidneybeans       0.91      1.00      0.95        20\n",
            "      lentil       0.47      0.73      0.57        11\n",
            "       maize       1.00      0.86      0.92        21\n",
            "       mango       0.86      1.00      0.93        19\n",
            "   mothbeans       0.94      0.67      0.78        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       0.89      1.00      0.94        17\n",
            "      orange       1.00      0.93      0.96        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       0.81      0.57      0.67        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.71      0.53      0.61        19\n",
            "  watermelon       1.00      0.89      0.94        19\n",
            "\n",
            "    accuracy                           0.90       440\n",
            "   macro avg       0.90      0.90      0.89       440\n",
            "weighted avg       0.91      0.90      0.90       440\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"soil_data.csv\")\n",
        "\n",
        "# Check the columns in the dataset to ensure they match\n",
        "print(\"Columns in the dataset:\", data.columns)\n",
        "\n",
        "# Feature selection - assuming N, P, K, ph, temperature, humidity are the input features\n",
        "X = data[['N', 'P', 'K', 'ph', 'temperature', 'humidity']]\n",
        "\n",
        "# Target variable - using 'label' as the column indicating the suitable crop\n",
        "y = data['label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling - Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the Neural Network (MLPClassifier)\n",
        "model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)  # You can adjust the number of neurons and iterations\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Function to recommend the best crop based on input soil characteristics\n",
        "def recommend_crop(N, P, K, ph, temperature, humidity):\n",
        "    input_data = pd.DataFrame([[N, P, K, ph, temperature, humidity]], columns=['N', 'P', 'K', 'ph', 'temperature', 'humidity'])\n",
        "    input_data_scaled = scaler.transform(input_data)  # Apply scaling to the input data\n",
        "    prediction = model.predict(input_data_scaled)  # Predict the crop\n",
        "    print(f\"Recommended crop for the given soil characteristics: {prediction[0]}\")\n",
        "\n",
        "# Example input for recommendation\n",
        "N = float(input(\"Enter Nitrogen content (N): \"))\n",
        "P = float(input(\"Enter Phosphorus content (P): \"))\n",
        "K = float(input(\"Enter Potassium content (K): \"))\n",
        "ph = float(input(\"Enter pH value: \"))\n",
        "temperature = float(input(\"Enter Temperature (°C): \"))\n",
        "humidity = float(input(\"Enter Humidity (%): \"))\n",
        "\n",
        "# Recommend the crop based on the input values\n",
        "recommend_crop(N, P, K, ph, temperature, humidity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egXHjqP0-8tU",
        "outputId": "08d917ff-fd4d-412e-e289-1823d6760ceb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the dataset: Index(['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'label'], dtype='object')\n",
            "Accuracy: 94.09%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       1.00      0.85      0.92        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       0.94      1.00      0.97        17\n",
            "      cotton       0.94      1.00      0.97        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.74      0.74      0.74        23\n",
            " kidneybeans       0.95      1.00      0.98        20\n",
            "      lentil       0.52      1.00      0.69        11\n",
            "       maize       1.00      0.95      0.98        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       1.00      0.79      0.88        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       0.90      0.78      0.84        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.72      0.68      0.70        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.94       440\n",
            "   macro avg       0.94      0.95      0.94       440\n",
            "weighted avg       0.95      0.94      0.94       440\n",
            "\n",
            "Enter Nitrogen content (N): 92\n",
            "Enter Phosphorus content (P): 85\n",
            "Enter Potassium content (K): 70\n",
            "Enter pH value: 7\n",
            "Enter Temperature (°C): 25\n",
            "Enter Humidity (%): 54\n",
            "Recommended crop for the given soil characteristics: jute\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"soil_data.csv\")\n",
        "\n",
        "# Check the columns in the dataset to ensure they match\n",
        "print(\"Columns in the dataset:\", data.columns)\n",
        "\n",
        "# Feature selection - assuming N, P, K, ph, temperature, humidity are the input features\n",
        "X = data[['N', 'P', 'K', 'ph', 'temperature', 'humidity']]\n",
        "\n",
        "# Target variable - using 'label' as the column indicating the suitable crop\n",
        "y = data['label']\n",
        "\n",
        "# Encode the target variable (crop labels) to numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling - Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the XGBoost model\n",
        "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Function to recommend the best crop based on input soil characteristics\n",
        "def recommend_crop(N, P, K, ph, temperature, moisture):\n",
        "    input_data = pd.DataFrame([[N, P, K, ph, temperature, moisture]], columns=['N', 'P', 'K', 'ph', 'temperature', 'humidity'])\n",
        "    input_data_scaled = scaler.transform(input_data)  # Apply scaling to the input data\n",
        "    prediction = model.predict(input_data_scaled)  # Predict the crop\n",
        "    predicted_crop = label_encoder.inverse_transform(prediction)  # Convert numeric prediction back to crop label\n",
        "    print(f\"Recommended crop for the given soil characteristics: {predicted_crop[0]}\")\n",
        "\n",
        "# Example input for recommendation\n",
        "n = 83\n",
        "p = 95\n",
        "K = 50\n",
        "temperature = 26.51\n",
        "moisture = 77.79\n",
        "ph = 5.50\n",
        "\n",
        "# Recommend the crop based on the input values\n",
        "recommend_crop(n, p, K, ph, temperature, moisture)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QctDU03RAYkp",
        "outputId": "8985fd4b-082e-4d92-d07d-d77d33d44206"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the dataset: Index(['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'label'], dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [08:55:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 96.59%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       1.00      0.95      0.97        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      0.96      0.98        27\n",
            "      coffee       1.00      1.00      1.00        17\n",
            "      cotton       0.94      1.00      0.97        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.83      0.87      0.85        23\n",
            " kidneybeans       1.00      1.00      1.00        20\n",
            "      lentil       0.79      1.00      0.88        11\n",
            "       maize       1.00      0.95      0.98        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       0.96      0.92      0.94        24\n",
            "    mungbean       0.95      1.00      0.97        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       0.91      0.87      0.89        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.83      0.79      0.81        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.97       440\n",
            "   macro avg       0.96      0.97      0.97       440\n",
            "weighted avg       0.97      0.97      0.97       440\n",
            "\n",
            "Recommended crop for the given soil characteristics: banana\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import lightgbm as lgb\n",
        "import warnings\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Suppress UserWarnings (like LightGBM training logs)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Suppress FutureWarnings (like scikit-learn deprecation)\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "data = pd.read_csv(\"soil_data.csv\")\n",
        "\n",
        "# Check the structure of the data to ensure it's loaded properly\n",
        "print(data.head())\n",
        "\n",
        "# Feature selection - assuming N, P, K, ph, temperature, humidity are the input features\n",
        "X = data[['N', 'P', 'K', 'ph', 'temperature', 'humidity']]\n",
        "\n",
        "# Target variable - using 'label' as the column indicating the suitable crop\n",
        "y = data['label']\n",
        "\n",
        "# Encode the target variable (crop labels) to numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling - Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the LightGBM model\n",
        "model = lgb.LGBMClassifier(\n",
        "    random_state=42,\n",
        "    num_leaves=31,   # Controls the complexity of the model\n",
        "    learning_rate=0.05,  # A common learning rate for better convergence\n",
        "    n_estimators=100,  # Number of boosting iterations\n",
        "    verbosity=-1  # Suppress LightGBM output to avoid log messages\n",
        ")\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Function to recommend the best crop based on input soil characteristics\n",
        "def recommend_crop(N, P, K, ph, temperature, moisture):\n",
        "    input_data = pd.DataFrame([[N, P, K, ph, temperature, moisture]], columns=['N', 'P', 'K', 'ph', 'temperature', 'humidity'])\n",
        "    input_data_scaled = scaler.transform(input_data)  # Apply scaling to the input data\n",
        "    prediction = model.predict(input_data_scaled)  # Predict the crop\n",
        "    predicted_crop = label_encoder.inverse_transform(prediction)  # Convert numeric prediction back to crop label\n",
        "    print(f\"Recommended crop for the given soil characteristics: {predicted_crop[0]}\")\n",
        "\n",
        "# Example input for recommendation\n",
        "n = 83\n",
        "p = 95\n",
        "K = 50\n",
        "temperature = 26.51\n",
        "moisture = 77.79\n",
        "ph = 5.50\n",
        "\n",
        "# Recommend the crop based on the input values\n",
        "recommend_crop(n, p, K, ph, temperature, moisture)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6H9Ca8xB0wV",
        "outputId": "a1b73a80-a732-4752-fed5-0b19ee2c453f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    N   P   K  temperature   humidity        ph label\n",
            "0  90  42  43    20.879744  82.002744  6.502985  rice\n",
            "1  85  58  41    21.770462  80.319644  7.038096  rice\n",
            "2  60  55  44    23.004459  82.320763  7.840207  rice\n",
            "3  74  35  40    26.491096  80.158363  6.980401  rice\n",
            "4  78  42  42    20.130175  81.604873  7.628473  rice\n",
            "Accuracy: 96.59%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       0.95      0.95      0.95        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      0.96      0.98        27\n",
            "      coffee       1.00      1.00      1.00        17\n",
            "      cotton       0.94      1.00      0.97        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.84      0.91      0.88        23\n",
            " kidneybeans       1.00      1.00      1.00        20\n",
            "      lentil       0.73      1.00      0.85        11\n",
            "       maize       1.00      0.95      0.98        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       1.00      0.88      0.93        24\n",
            "    mungbean       1.00      0.95      0.97        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       0.88      0.91      0.89        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.88      0.79      0.83        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.97       440\n",
            "   macro avg       0.96      0.97      0.97       440\n",
            "weighted avg       0.97      0.97      0.97       440\n",
            "\n",
            "Recommended crop for the given soil characteristics: banana\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import warnings\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Suppress UserWarnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Suppress FutureWarnings\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "data = pd.read_csv(\"soil_data.csv\")\n",
        "\n",
        "# Check the structure of the data to ensure it's loaded properly\n",
        "print(data.head())\n",
        "\n",
        "# Feature selection - assuming N, P, K, ph, temperature, humidity are the input features\n",
        "X = data[['N', 'P', 'K', 'ph', 'temperature', 'humidity']]\n",
        "\n",
        "# Target variable - using 'label' as the column indicating the suitable crop\n",
        "y = data['label']\n",
        "\n",
        "# Encode the target variable (crop labels) to numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling - Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the Random Forest Classifier model\n",
        "model = RandomForestClassifier(\n",
        "    random_state=42,\n",
        "    n_estimators=100,  # Number of trees in the forest\n",
        "    max_depth=None,  # No limit on the depth of trees\n",
        "    min_samples_split=2,  # Minimum number of samples to split a node\n",
        "    min_samples_leaf=1,  # Minimum number of samples to be at a leaf node\n",
        "    n_jobs=-1  # Use all cores for training\n",
        ")\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Function to recommend the best crop based on input soil characteristics\n",
        "def recommend_crop(N, P, K, ph, temperature, moisture):\n",
        "    input_data = pd.DataFrame([[N, P, K, ph, temperature, moisture]], columns=['N', 'P', 'K', 'ph', 'temperature', 'humidity'])\n",
        "    input_data_scaled = scaler.transform(input_data)  # Apply scaling to the input data\n",
        "    prediction = model.predict(input_data_scaled)  # Predict the crop label (numeric value)\n",
        "    predicted_crop = label_encoder.inverse_transform([prediction[0]])  # Convert numeric prediction back to crop label\n",
        "    print(f\"Recommended crop for the given soil characteristics: {predicted_crop[0]}\")\n",
        "\n",
        "# Example input for recommendation\n",
        "n = 83\n",
        "p = 95\n",
        "K = 50\n",
        "temperature = 26.51\n",
        "moisture = 77.79\n",
        "ph = 5.50\n",
        "\n",
        "# Recommend the crop based on the input values\n",
        "recommend_crop(n, p, K, ph, temperature, moisture)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnE5_2NeEbw_",
        "outputId": "8917be9d-1bb0-4ed1-f5fd-db9fbe7f9c3e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    N   P   K  temperature   humidity        ph label\n",
            "0  90  42  43    20.879744  82.002744  6.502985  rice\n",
            "1  85  58  41    21.770462  80.319644  7.038096  rice\n",
            "2  60  55  44    23.004459  82.320763  7.840207  rice\n",
            "3  74  35  40    26.491096  80.158363  6.980401  rice\n",
            "4  78  42  42    20.130175  81.604873  7.628473  rice\n",
            "Accuracy: 97.50%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       1.00      0.95      0.97        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      1.00      1.00        27\n",
            "      coffee       1.00      1.00      1.00        17\n",
            "      cotton       1.00      1.00      1.00        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.88      0.91      0.89        23\n",
            " kidneybeans       1.00      1.00      1.00        20\n",
            "      lentil       0.73      1.00      0.85        11\n",
            "       maize       1.00      1.00      1.00        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       1.00      0.88      0.93        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       0.91      0.91      0.91        23\n",
            " pomegranate       1.00      1.00      1.00        23\n",
            "        rice       0.89      0.84      0.86        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.97       440\n",
            "   macro avg       0.97      0.98      0.97       440\n",
            "weighted avg       0.98      0.97      0.98       440\n",
            "\n",
            "Recommended crop for the given soil characteristics: banana\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import warnings\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Suppress UserWarnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Suppress FutureWarnings\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "data = pd.read_csv(\"soil_data.csv\")\n",
        "\n",
        "# Check the structure of the data to ensure it's loaded properly\n",
        "print(data.head())\n",
        "\n",
        "# Feature selection - assuming N, P, K, ph, temperature, humidity are the input features\n",
        "X = data[['N', 'P', 'K', 'ph', 'temperature', 'humidity']]\n",
        "\n",
        "# Target variable - using 'label' as the column indicating the suitable crop\n",
        "y = data['label']\n",
        "\n",
        "# Encode the target variable (crop labels) to numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling - Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the Gradient Boosting Regressor model\n",
        "model = GradientBoostingRegressor(\n",
        "    n_estimators=100,  # Number of boosting stages to perform\n",
        "    learning_rate=0.1,  # The contribution of each tree to the final model\n",
        "    max_depth=3,  # Maximum depth of the individual trees\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model performance\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "print(f\"R-squared: {r2}\")\n",
        "\n",
        "# Function to recommend the best crop based on input soil characteristics\n",
        "def recommend_crop(N, P, K, ph, temperature, moisture):\n",
        "    input_data = pd.DataFrame([[N, P, K, ph, temperature, moisture]], columns=['N', 'P', 'K', 'ph', 'temperature', 'humidity'])\n",
        "    input_data_scaled = scaler.transform(input_data)  # Apply scaling to the input data\n",
        "    prediction = model.predict(input_data_scaled)  # Predict the crop label (numeric value)\n",
        "    predicted_crop = label_encoder.inverse_transform([int(round(prediction[0]))])  # Convert numeric prediction back to crop label\n",
        "    print(f\"Recommended crop for the given soil characteristics: {predicted_crop[0]}\")\n",
        "\n",
        "# Example input for recommendation\n",
        "n = 83\n",
        "p = 95\n",
        "K = 50\n",
        "temperature = 26.51\n",
        "moisture = 77.79\n",
        "ph = 5.50\n",
        "\n",
        "# Recommend the crop based on the input values\n",
        "recommend_crop(n, p, K, ph, temperature, moisture)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc3onnv8FkFo",
        "outputId": "69d503c6-c25a-4373-ffc3-73f6ee35ad79"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    N   P   K  temperature   humidity        ph label\n",
            "0  90  42  43    20.879744  82.002744  6.502985  rice\n",
            "1  85  58  41    21.770462  80.319644  7.038096  rice\n",
            "2  60  55  44    23.004459  82.320763  7.840207  rice\n",
            "3  74  35  40    26.491096  80.158363  6.980401  rice\n",
            "4  78  42  42    20.130175  81.604873  7.628473  rice\n",
            "Mean Absolute Error: 1.5948408437228512\n",
            "R-squared: 0.8509045675986933\n",
            "Recommended crop for the given soil characteristics: blackgram\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import warnings\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Suppress UserWarnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Suppress FutureWarnings\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "data = pd.read_csv(\"soil_data.csv\")\n",
        "\n",
        "# Check the structure of the data to ensure it's loaded properly\n",
        "print(data.head())\n",
        "\n",
        "# Feature selection - assuming N, P, K, ph, temperature, humidity are the input features\n",
        "X = data[['N', 'P', 'K', 'ph', 'temperature', 'humidity']]\n",
        "\n",
        "# Target variable - using 'label' as the column indicating the suitable crop\n",
        "y = data['label']\n",
        "\n",
        "# Encode the target variable (crop labels) to numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling - Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the Gradient Boosting Classifier model\n",
        "model = GradientBoostingClassifier(\n",
        "    n_estimators=100,  # Number of boosting stages to perform\n",
        "    learning_rate=0.1,  # The contribution of each tree to the final model\n",
        "    max_depth=3,  # Maximum depth of the individual trees\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Classification report (precision, recall, f1-score)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Function to recommend the best crop based on input soil characteristics\n",
        "def recommend_crop(N, P, K, ph, temperature, moisture):\n",
        "    input_data = pd.DataFrame([[N, P, K, ph, temperature, moisture]], columns=['N', 'P', 'K', 'ph', 'temperature', 'humidity'])\n",
        "    input_data_scaled = scaler.transform(input_data)  # Apply scaling to the input data\n",
        "    prediction = model.predict(input_data_scaled)  # Predict the crop label (numeric value)\n",
        "    predicted_crop = label_encoder.inverse_transform([prediction[0]])  # Convert numeric prediction back to crop label\n",
        "    print(f\"Recommended crop for the given soil characteristics: {predicted_crop[0]}\")\n",
        "\n",
        "# Example input for recommendation\n",
        "n = 83\n",
        "p = 95\n",
        "K = 50\n",
        "temperature = 26.51\n",
        "moisture = 77.79\n",
        "ph = 5.50\n",
        "\n",
        "# Recommend the crop based on the input values\n",
        "recommend_crop(n, p, K, ph, temperature, moisture)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoA0vU-2FxHl",
        "outputId": "abadd267-b9e7-4f2a-8093-17a626708ce5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    N   P   K  temperature   humidity        ph label\n",
            "0  90  42  43    20.879744  82.002744  6.502985  rice\n",
            "1  85  58  41    21.770462  80.319644  7.038096  rice\n",
            "2  60  55  44    23.004459  82.320763  7.840207  rice\n",
            "3  74  35  40    26.491096  80.158363  6.980401  rice\n",
            "4  78  42  42    20.130175  81.604873  7.628473  rice\n",
            "Accuracy: 95.68%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        23\n",
            "      banana       1.00      1.00      1.00        21\n",
            "   blackgram       1.00      0.95      0.97        20\n",
            "    chickpea       1.00      1.00      1.00        26\n",
            "     coconut       1.00      0.96      0.98        27\n",
            "      coffee       1.00      1.00      1.00        17\n",
            "      cotton       1.00      1.00      1.00        17\n",
            "      grapes       1.00      1.00      1.00        14\n",
            "        jute       0.78      0.91      0.84        23\n",
            " kidneybeans       1.00      1.00      1.00        20\n",
            "      lentil       0.61      1.00      0.76        11\n",
            "       maize       1.00      1.00      1.00        21\n",
            "       mango       1.00      1.00      1.00        19\n",
            "   mothbeans       0.91      0.83      0.87        24\n",
            "    mungbean       1.00      1.00      1.00        19\n",
            "   muskmelon       1.00      1.00      1.00        17\n",
            "      orange       1.00      1.00      1.00        14\n",
            "      papaya       1.00      1.00      1.00        23\n",
            "  pigeonpeas       0.90      0.78      0.84        23\n",
            " pomegranate       1.00      0.91      0.95        23\n",
            "        rice       0.88      0.79      0.83        19\n",
            "  watermelon       1.00      1.00      1.00        19\n",
            "\n",
            "    accuracy                           0.96       440\n",
            "   macro avg       0.96      0.96      0.96       440\n",
            "weighted avg       0.96      0.96      0.96       440\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0 19  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 26  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0 26  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0 21  0  0  0  0  0  0  0  0  0  0  0  2  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0 21  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0 19  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  2  0  0 20  0  0  0  0  2  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 19  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 23  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  3  0  0  2  0  0  0  0 18  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0 21  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0 15  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19]]\n",
            "Recommended crop for the given soil characteristics: banana\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "df = pd.read_csv('soil_data.csv')\n",
        "\n",
        "# Check the first few rows of the dataset to understand its structure\n",
        "print(df.head())\n",
        "\n",
        "# Encode the target labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "# Features and target\n",
        "X = df.drop('label', axis=1)  # Features\n",
        "y = df['label']  # Target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the CatBoost model\n",
        "model = CatBoostClassifier(iterations=500, learning_rate=0.1, depth=6, random_state=42, cat_features=[])\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Accuracy on test set\n",
        "accuracy = model.score(X_test_scaled, y_test) * 100\n",
        "print(f\"Model Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Function to predict crop based on user input\n",
        "def recommend_crop(N, P, K, temperature, humidity, ph):\n",
        "    # Prepare the input features in the same format as the training data\n",
        "    input_data = pd.DataFrame([[N, P, K, temperature, humidity, ph]], columns=X.columns)\n",
        "\n",
        "    # Scale the input features\n",
        "    input_scaled = scaler.transform(input_data)\n",
        "\n",
        "    # Predict the label\n",
        "    predicted_label = model.predict(input_scaled)[0]\n",
        "\n",
        "    # Convert the numeric label back to the crop name\n",
        "    predicted_crop = label_encoder.inverse_transform([predicted_label])[0]\n",
        "\n",
        "    return predicted_crop\n",
        "\n",
        "# Example usage - replace with user input or interactive method\n",
        "N = float(input(\"Enter Nitrogen (N) value: \"))\n",
        "P = float(input(\"Enter Phosphorus (P) value: \"))\n",
        "K = float(input(\"Enter Potassium (K) value: \"))\n",
        "temperature = float(input(\"Enter temperature value: \"))\n",
        "humidity = float(input(\"Enter humidity value: \"))\n",
        "ph = float(input(\"Enter pH value: \"))\n",
        "\n",
        "# Recommend crop\n",
        "recommended_crop = recommend_crop(N, P, K, temperature, humidity, ph)\n",
        "print(f\"The recommended crop is: {recommended_crop}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nodUbKrzGIP0",
        "outputId": "ec42e5cd-bdaa-42c3-bf3f-f2aebf481621"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    N   P   K  temperature   humidity        ph label\n",
            "0  90  42  43    20.879744  82.002744  6.502985  rice\n",
            "1  85  58  41    21.770462  80.319644  7.038096  rice\n",
            "2  60  55  44    23.004459  82.320763  7.840207  rice\n",
            "3  74  35  40    26.491096  80.158363  6.980401  rice\n",
            "4  78  42  42    20.130175  81.604873  7.628473  rice\n",
            "0:\tlearn: 2.4979295\ttotal: 67.6ms\tremaining: 33.7s\n",
            "1:\tlearn: 2.1828617\ttotal: 98.2ms\tremaining: 24.5s\n",
            "2:\tlearn: 1.8875509\ttotal: 133ms\tremaining: 22s\n",
            "3:\tlearn: 1.6451391\ttotal: 169ms\tremaining: 21s\n",
            "4:\tlearn: 1.4695191\ttotal: 202ms\tremaining: 20s\n",
            "5:\tlearn: 1.3318852\ttotal: 272ms\tremaining: 22.4s\n",
            "6:\tlearn: 1.1874715\ttotal: 303ms\tremaining: 21.4s\n",
            "7:\tlearn: 1.0800975\ttotal: 346ms\tremaining: 21.3s\n",
            "8:\tlearn: 0.9916743\ttotal: 393ms\tremaining: 21.4s\n",
            "9:\tlearn: 0.9134332\ttotal: 431ms\tremaining: 21.1s\n",
            "10:\tlearn: 0.8467523\ttotal: 485ms\tremaining: 21.6s\n",
            "11:\tlearn: 0.7802408\ttotal: 526ms\tremaining: 21.4s\n",
            "12:\tlearn: 0.7323859\ttotal: 563ms\tremaining: 21.1s\n",
            "13:\tlearn: 0.6839107\ttotal: 608ms\tremaining: 21.1s\n",
            "14:\tlearn: 0.6412518\ttotal: 640ms\tremaining: 20.7s\n",
            "15:\tlearn: 0.6052899\ttotal: 676ms\tremaining: 20.4s\n",
            "16:\tlearn: 0.5735039\ttotal: 729ms\tremaining: 20.7s\n",
            "17:\tlearn: 0.5430972\ttotal: 766ms\tremaining: 20.5s\n",
            "18:\tlearn: 0.5158739\ttotal: 793ms\tremaining: 20.1s\n",
            "19:\tlearn: 0.4892351\ttotal: 823ms\tremaining: 19.8s\n",
            "20:\tlearn: 0.4698154\ttotal: 864ms\tremaining: 19.7s\n",
            "21:\tlearn: 0.4476648\ttotal: 908ms\tremaining: 19.7s\n",
            "22:\tlearn: 0.4260555\ttotal: 946ms\tremaining: 19.6s\n",
            "23:\tlearn: 0.4083970\ttotal: 1.02s\tremaining: 20.2s\n",
            "24:\tlearn: 0.3912741\ttotal: 1.09s\tremaining: 20.7s\n",
            "25:\tlearn: 0.3744732\ttotal: 1.19s\tremaining: 21.8s\n",
            "26:\tlearn: 0.3582094\ttotal: 1.27s\tremaining: 22.2s\n",
            "27:\tlearn: 0.3424627\ttotal: 1.32s\tremaining: 22.3s\n",
            "28:\tlearn: 0.3305747\ttotal: 1.38s\tremaining: 22.4s\n",
            "29:\tlearn: 0.3202671\ttotal: 1.44s\tremaining: 22.6s\n",
            "30:\tlearn: 0.3098349\ttotal: 1.5s\tremaining: 22.7s\n",
            "31:\tlearn: 0.3001533\ttotal: 1.55s\tremaining: 22.7s\n",
            "32:\tlearn: 0.2904332\ttotal: 1.61s\tremaining: 22.8s\n",
            "33:\tlearn: 0.2819603\ttotal: 1.69s\tremaining: 23.2s\n",
            "34:\tlearn: 0.2734118\ttotal: 1.81s\tremaining: 24s\n",
            "35:\tlearn: 0.2655383\ttotal: 1.91s\tremaining: 24.7s\n",
            "36:\tlearn: 0.2592334\ttotal: 1.99s\tremaining: 24.9s\n",
            "37:\tlearn: 0.2525846\ttotal: 2.06s\tremaining: 25s\n",
            "38:\tlearn: 0.2470771\ttotal: 2.11s\tremaining: 24.9s\n",
            "39:\tlearn: 0.2403201\ttotal: 2.15s\tremaining: 24.7s\n",
            "40:\tlearn: 0.2350344\ttotal: 2.19s\tremaining: 24.6s\n",
            "41:\tlearn: 0.2281771\ttotal: 2.23s\tremaining: 24.4s\n",
            "42:\tlearn: 0.2232815\ttotal: 2.28s\tremaining: 24.3s\n",
            "43:\tlearn: 0.2174920\ttotal: 2.32s\tremaining: 24.1s\n",
            "44:\tlearn: 0.2127781\ttotal: 2.38s\tremaining: 24s\n",
            "45:\tlearn: 0.2085557\ttotal: 2.43s\tremaining: 23.9s\n",
            "46:\tlearn: 0.2049605\ttotal: 2.48s\tremaining: 23.9s\n",
            "47:\tlearn: 0.2012488\ttotal: 2.54s\tremaining: 23.9s\n",
            "48:\tlearn: 0.1972195\ttotal: 2.63s\tremaining: 24.2s\n",
            "49:\tlearn: 0.1932914\ttotal: 2.7s\tremaining: 24.3s\n",
            "50:\tlearn: 0.1893757\ttotal: 2.76s\tremaining: 24.3s\n",
            "51:\tlearn: 0.1853078\ttotal: 2.83s\tremaining: 24.4s\n",
            "52:\tlearn: 0.1820984\ttotal: 2.89s\tremaining: 24.4s\n",
            "53:\tlearn: 0.1790538\ttotal: 2.94s\tremaining: 24.3s\n",
            "54:\tlearn: 0.1761274\ttotal: 3s\tremaining: 24.3s\n",
            "55:\tlearn: 0.1729912\ttotal: 3.07s\tremaining: 24.4s\n",
            "56:\tlearn: 0.1702776\ttotal: 3.13s\tremaining: 24.3s\n",
            "57:\tlearn: 0.1681661\ttotal: 3.17s\tremaining: 24.2s\n",
            "58:\tlearn: 0.1659355\ttotal: 3.23s\tremaining: 24.1s\n",
            "59:\tlearn: 0.1632019\ttotal: 3.27s\tremaining: 24s\n",
            "60:\tlearn: 0.1606534\ttotal: 3.31s\tremaining: 23.8s\n",
            "61:\tlearn: 0.1587824\ttotal: 3.37s\tremaining: 23.8s\n",
            "62:\tlearn: 0.1560582\ttotal: 3.45s\tremaining: 23.9s\n",
            "63:\tlearn: 0.1541736\ttotal: 3.53s\tremaining: 24s\n",
            "64:\tlearn: 0.1521247\ttotal: 3.59s\tremaining: 24s\n",
            "65:\tlearn: 0.1499472\ttotal: 3.64s\tremaining: 23.9s\n",
            "66:\tlearn: 0.1480370\ttotal: 3.68s\tremaining: 23.8s\n",
            "67:\tlearn: 0.1453948\ttotal: 3.77s\tremaining: 23.9s\n",
            "68:\tlearn: 0.1434290\ttotal: 3.83s\tremaining: 23.9s\n",
            "69:\tlearn: 0.1422080\ttotal: 3.9s\tremaining: 23.9s\n",
            "70:\tlearn: 0.1406453\ttotal: 3.96s\tremaining: 23.9s\n",
            "71:\tlearn: 0.1387646\ttotal: 4.02s\tremaining: 23.9s\n",
            "72:\tlearn: 0.1374568\ttotal: 4.1s\tremaining: 24s\n",
            "73:\tlearn: 0.1359720\ttotal: 4.18s\tremaining: 24.1s\n",
            "74:\tlearn: 0.1340010\ttotal: 4.25s\tremaining: 24.1s\n",
            "75:\tlearn: 0.1319110\ttotal: 4.33s\tremaining: 24.2s\n",
            "76:\tlearn: 0.1305927\ttotal: 4.42s\tremaining: 24.3s\n",
            "77:\tlearn: 0.1295304\ttotal: 4.48s\tremaining: 24.3s\n",
            "78:\tlearn: 0.1282922\ttotal: 4.57s\tremaining: 24.4s\n",
            "79:\tlearn: 0.1268522\ttotal: 4.62s\tremaining: 24.2s\n",
            "80:\tlearn: 0.1256116\ttotal: 4.66s\tremaining: 24.1s\n",
            "81:\tlearn: 0.1238316\ttotal: 4.69s\tremaining: 23.9s\n",
            "82:\tlearn: 0.1226486\ttotal: 4.76s\tremaining: 23.9s\n",
            "83:\tlearn: 0.1212990\ttotal: 4.83s\tremaining: 23.9s\n",
            "84:\tlearn: 0.1204803\ttotal: 4.89s\tremaining: 23.9s\n",
            "85:\tlearn: 0.1191803\ttotal: 4.93s\tremaining: 23.7s\n",
            "86:\tlearn: 0.1179725\ttotal: 4.98s\tremaining: 23.6s\n",
            "87:\tlearn: 0.1170773\ttotal: 5.02s\tremaining: 23.5s\n",
            "88:\tlearn: 0.1162477\ttotal: 5.06s\tremaining: 23.3s\n",
            "89:\tlearn: 0.1151897\ttotal: 5.09s\tremaining: 23.2s\n",
            "90:\tlearn: 0.1139032\ttotal: 5.13s\tremaining: 23.1s\n",
            "91:\tlearn: 0.1130303\ttotal: 5.21s\tremaining: 23.1s\n",
            "92:\tlearn: 0.1122236\ttotal: 5.26s\tremaining: 23s\n",
            "93:\tlearn: 0.1109441\ttotal: 5.31s\tremaining: 23s\n",
            "94:\tlearn: 0.1100136\ttotal: 5.37s\tremaining: 22.9s\n",
            "95:\tlearn: 0.1088622\ttotal: 5.45s\tremaining: 22.9s\n",
            "96:\tlearn: 0.1078401\ttotal: 5.5s\tremaining: 22.9s\n",
            "97:\tlearn: 0.1069314\ttotal: 5.57s\tremaining: 22.8s\n",
            "98:\tlearn: 0.1059038\ttotal: 5.62s\tremaining: 22.8s\n",
            "99:\tlearn: 0.1049334\ttotal: 5.69s\tremaining: 22.8s\n",
            "100:\tlearn: 0.1035084\ttotal: 5.77s\tremaining: 22.8s\n",
            "101:\tlearn: 0.1029674\ttotal: 5.8s\tremaining: 22.6s\n",
            "102:\tlearn: 0.1020021\ttotal: 5.85s\tremaining: 22.5s\n",
            "103:\tlearn: 0.1011517\ttotal: 5.88s\tremaining: 22.4s\n",
            "104:\tlearn: 0.1002147\ttotal: 5.94s\tremaining: 22.4s\n",
            "105:\tlearn: 0.0994946\ttotal: 5.97s\tremaining: 22.2s\n",
            "106:\tlearn: 0.0987049\ttotal: 6.01s\tremaining: 22.1s\n",
            "107:\tlearn: 0.0981795\ttotal: 6.05s\tremaining: 22s\n",
            "108:\tlearn: 0.0974642\ttotal: 6.1s\tremaining: 21.9s\n",
            "109:\tlearn: 0.0967125\ttotal: 6.15s\tremaining: 21.8s\n",
            "110:\tlearn: 0.0959687\ttotal: 6.19s\tremaining: 21.7s\n",
            "111:\tlearn: 0.0953959\ttotal: 6.22s\tremaining: 21.6s\n",
            "112:\tlearn: 0.0948329\ttotal: 6.26s\tremaining: 21.5s\n",
            "113:\tlearn: 0.0942863\ttotal: 6.31s\tremaining: 21.4s\n",
            "114:\tlearn: 0.0935834\ttotal: 6.34s\tremaining: 21.2s\n",
            "115:\tlearn: 0.0931451\ttotal: 6.38s\tremaining: 21.1s\n",
            "116:\tlearn: 0.0926829\ttotal: 6.42s\tremaining: 21s\n",
            "117:\tlearn: 0.0921631\ttotal: 6.46s\tremaining: 20.9s\n",
            "118:\tlearn: 0.0913903\ttotal: 6.49s\tremaining: 20.8s\n",
            "119:\tlearn: 0.0908470\ttotal: 6.55s\tremaining: 20.7s\n",
            "120:\tlearn: 0.0900590\ttotal: 6.59s\tremaining: 20.6s\n",
            "121:\tlearn: 0.0895932\ttotal: 6.63s\tremaining: 20.5s\n",
            "122:\tlearn: 0.0891965\ttotal: 6.67s\tremaining: 20.4s\n",
            "123:\tlearn: 0.0884545\ttotal: 6.71s\tremaining: 20.3s\n",
            "124:\tlearn: 0.0878219\ttotal: 6.74s\tremaining: 20.2s\n",
            "125:\tlearn: 0.0868578\ttotal: 6.78s\tremaining: 20.1s\n",
            "126:\tlearn: 0.0864714\ttotal: 6.81s\tremaining: 20s\n",
            "127:\tlearn: 0.0859106\ttotal: 6.85s\tremaining: 19.9s\n",
            "128:\tlearn: 0.0853232\ttotal: 6.87s\tremaining: 19.8s\n",
            "129:\tlearn: 0.0841435\ttotal: 6.91s\tremaining: 19.7s\n",
            "130:\tlearn: 0.0836120\ttotal: 6.95s\tremaining: 19.6s\n",
            "131:\tlearn: 0.0830929\ttotal: 6.99s\tremaining: 19.5s\n",
            "132:\tlearn: 0.0824957\ttotal: 7.03s\tremaining: 19.4s\n",
            "133:\tlearn: 0.0819381\ttotal: 7.07s\tremaining: 19.3s\n",
            "134:\tlearn: 0.0813422\ttotal: 7.11s\tremaining: 19.2s\n",
            "135:\tlearn: 0.0807029\ttotal: 7.16s\tremaining: 19.2s\n",
            "136:\tlearn: 0.0800244\ttotal: 7.21s\tremaining: 19.1s\n",
            "137:\tlearn: 0.0795643\ttotal: 7.25s\tremaining: 19s\n",
            "138:\tlearn: 0.0790565\ttotal: 7.29s\tremaining: 18.9s\n",
            "139:\tlearn: 0.0786543\ttotal: 7.33s\tremaining: 18.9s\n",
            "140:\tlearn: 0.0783109\ttotal: 7.37s\tremaining: 18.8s\n",
            "141:\tlearn: 0.0778118\ttotal: 7.42s\tremaining: 18.7s\n",
            "142:\tlearn: 0.0772870\ttotal: 7.45s\tremaining: 18.6s\n",
            "143:\tlearn: 0.0767897\ttotal: 7.49s\tremaining: 18.5s\n",
            "144:\tlearn: 0.0765306\ttotal: 7.54s\tremaining: 18.5s\n",
            "145:\tlearn: 0.0761043\ttotal: 7.6s\tremaining: 18.4s\n",
            "146:\tlearn: 0.0756266\ttotal: 7.65s\tremaining: 18.4s\n",
            "147:\tlearn: 0.0752859\ttotal: 7.69s\tremaining: 18.3s\n",
            "148:\tlearn: 0.0748188\ttotal: 7.73s\tremaining: 18.2s\n",
            "149:\tlearn: 0.0744202\ttotal: 7.77s\tremaining: 18.1s\n",
            "150:\tlearn: 0.0741385\ttotal: 7.83s\tremaining: 18.1s\n",
            "151:\tlearn: 0.0737651\ttotal: 7.87s\tremaining: 18s\n",
            "152:\tlearn: 0.0732382\ttotal: 7.9s\tremaining: 17.9s\n",
            "153:\tlearn: 0.0728667\ttotal: 7.92s\tremaining: 17.8s\n",
            "154:\tlearn: 0.0723992\ttotal: 7.97s\tremaining: 17.7s\n",
            "155:\tlearn: 0.0720817\ttotal: 8s\tremaining: 17.6s\n",
            "156:\tlearn: 0.0718819\ttotal: 8.04s\tremaining: 17.6s\n",
            "157:\tlearn: 0.0715660\ttotal: 8.08s\tremaining: 17.5s\n",
            "158:\tlearn: 0.0714177\ttotal: 8.13s\tremaining: 17.4s\n",
            "159:\tlearn: 0.0711192\ttotal: 8.17s\tremaining: 17.4s\n",
            "160:\tlearn: 0.0706906\ttotal: 8.21s\tremaining: 17.3s\n",
            "161:\tlearn: 0.0702725\ttotal: 8.25s\tremaining: 17.2s\n",
            "162:\tlearn: 0.0698683\ttotal: 8.32s\tremaining: 17.2s\n",
            "163:\tlearn: 0.0694195\ttotal: 8.4s\tremaining: 17.2s\n",
            "164:\tlearn: 0.0688995\ttotal: 8.46s\tremaining: 17.2s\n",
            "165:\tlearn: 0.0685040\ttotal: 8.53s\tremaining: 17.2s\n",
            "166:\tlearn: 0.0682134\ttotal: 8.61s\tremaining: 17.2s\n",
            "167:\tlearn: 0.0677649\ttotal: 8.67s\tremaining: 17.1s\n",
            "168:\tlearn: 0.0673738\ttotal: 8.72s\tremaining: 17.1s\n",
            "169:\tlearn: 0.0668436\ttotal: 8.79s\tremaining: 17.1s\n",
            "170:\tlearn: 0.0663499\ttotal: 8.87s\tremaining: 17.1s\n",
            "171:\tlearn: 0.0661664\ttotal: 8.91s\tremaining: 17s\n",
            "172:\tlearn: 0.0657619\ttotal: 8.98s\tremaining: 17s\n",
            "173:\tlearn: 0.0655380\ttotal: 9.03s\tremaining: 16.9s\n",
            "174:\tlearn: 0.0652227\ttotal: 9.06s\tremaining: 16.8s\n",
            "175:\tlearn: 0.0649072\ttotal: 9.08s\tremaining: 16.7s\n",
            "176:\tlearn: 0.0645722\ttotal: 9.1s\tremaining: 16.6s\n",
            "177:\tlearn: 0.0643689\ttotal: 9.12s\tremaining: 16.5s\n",
            "178:\tlearn: 0.0640838\ttotal: 9.14s\tremaining: 16.4s\n",
            "179:\tlearn: 0.0637370\ttotal: 9.16s\tremaining: 16.3s\n",
            "180:\tlearn: 0.0633860\ttotal: 9.18s\tremaining: 16.2s\n",
            "181:\tlearn: 0.0630858\ttotal: 9.2s\tremaining: 16.1s\n",
            "182:\tlearn: 0.0627456\ttotal: 9.22s\tremaining: 16s\n",
            "183:\tlearn: 0.0625164\ttotal: 9.25s\tremaining: 15.9s\n",
            "184:\tlearn: 0.0620373\ttotal: 9.27s\tremaining: 15.8s\n",
            "185:\tlearn: 0.0617581\ttotal: 9.29s\tremaining: 15.7s\n",
            "186:\tlearn: 0.0614740\ttotal: 9.31s\tremaining: 15.6s\n",
            "187:\tlearn: 0.0610631\ttotal: 9.34s\tremaining: 15.5s\n",
            "188:\tlearn: 0.0608045\ttotal: 9.36s\tremaining: 15.4s\n",
            "189:\tlearn: 0.0604083\ttotal: 9.38s\tremaining: 15.3s\n",
            "190:\tlearn: 0.0601434\ttotal: 9.4s\tremaining: 15.2s\n",
            "191:\tlearn: 0.0597566\ttotal: 9.42s\tremaining: 15.1s\n",
            "192:\tlearn: 0.0594337\ttotal: 9.44s\tremaining: 15s\n",
            "193:\tlearn: 0.0591696\ttotal: 9.46s\tremaining: 14.9s\n",
            "194:\tlearn: 0.0589121\ttotal: 9.48s\tremaining: 14.8s\n",
            "195:\tlearn: 0.0587184\ttotal: 9.5s\tremaining: 14.7s\n",
            "196:\tlearn: 0.0585013\ttotal: 9.53s\tremaining: 14.7s\n",
            "197:\tlearn: 0.0581137\ttotal: 9.55s\tremaining: 14.6s\n",
            "198:\tlearn: 0.0578331\ttotal: 9.57s\tremaining: 14.5s\n",
            "199:\tlearn: 0.0576258\ttotal: 9.59s\tremaining: 14.4s\n",
            "200:\tlearn: 0.0574081\ttotal: 9.62s\tremaining: 14.3s\n",
            "201:\tlearn: 0.0571713\ttotal: 9.64s\tremaining: 14.2s\n",
            "202:\tlearn: 0.0570077\ttotal: 9.67s\tremaining: 14.1s\n",
            "203:\tlearn: 0.0566922\ttotal: 9.69s\tremaining: 14.1s\n",
            "204:\tlearn: 0.0564159\ttotal: 9.71s\tremaining: 14s\n",
            "205:\tlearn: 0.0562392\ttotal: 9.74s\tremaining: 13.9s\n",
            "206:\tlearn: 0.0558715\ttotal: 9.76s\tremaining: 13.8s\n",
            "207:\tlearn: 0.0555406\ttotal: 9.79s\tremaining: 13.7s\n",
            "208:\tlearn: 0.0553725\ttotal: 9.81s\tremaining: 13.7s\n",
            "209:\tlearn: 0.0551600\ttotal: 9.83s\tremaining: 13.6s\n",
            "210:\tlearn: 0.0549037\ttotal: 9.85s\tremaining: 13.5s\n",
            "211:\tlearn: 0.0545823\ttotal: 9.87s\tremaining: 13.4s\n",
            "212:\tlearn: 0.0544389\ttotal: 9.89s\tremaining: 13.3s\n",
            "213:\tlearn: 0.0541632\ttotal: 9.91s\tremaining: 13.2s\n",
            "214:\tlearn: 0.0538210\ttotal: 9.93s\tremaining: 13.2s\n",
            "215:\tlearn: 0.0536202\ttotal: 9.95s\tremaining: 13.1s\n",
            "216:\tlearn: 0.0533530\ttotal: 9.97s\tremaining: 13s\n",
            "217:\tlearn: 0.0531103\ttotal: 10s\tremaining: 12.9s\n",
            "218:\tlearn: 0.0528988\ttotal: 10s\tremaining: 12.9s\n",
            "219:\tlearn: 0.0526782\ttotal: 10s\tremaining: 12.8s\n",
            "220:\tlearn: 0.0525698\ttotal: 10.1s\tremaining: 12.7s\n",
            "221:\tlearn: 0.0523600\ttotal: 10.1s\tremaining: 12.6s\n",
            "222:\tlearn: 0.0521448\ttotal: 10.1s\tremaining: 12.5s\n",
            "223:\tlearn: 0.0518964\ttotal: 10.1s\tremaining: 12.5s\n",
            "224:\tlearn: 0.0517283\ttotal: 10.2s\tremaining: 12.4s\n",
            "225:\tlearn: 0.0515306\ttotal: 10.2s\tremaining: 12.3s\n",
            "226:\tlearn: 0.0512841\ttotal: 10.2s\tremaining: 12.3s\n",
            "227:\tlearn: 0.0508895\ttotal: 10.2s\tremaining: 12.2s\n",
            "228:\tlearn: 0.0507636\ttotal: 10.2s\tremaining: 12.1s\n",
            "229:\tlearn: 0.0505672\ttotal: 10.3s\tremaining: 12s\n",
            "230:\tlearn: 0.0502688\ttotal: 10.3s\tremaining: 12s\n",
            "231:\tlearn: 0.0499428\ttotal: 10.3s\tremaining: 11.9s\n",
            "232:\tlearn: 0.0498132\ttotal: 10.3s\tremaining: 11.8s\n",
            "233:\tlearn: 0.0496243\ttotal: 10.3s\tremaining: 11.8s\n",
            "234:\tlearn: 0.0494542\ttotal: 10.4s\tremaining: 11.7s\n",
            "235:\tlearn: 0.0492109\ttotal: 10.4s\tremaining: 11.6s\n",
            "236:\tlearn: 0.0490287\ttotal: 10.4s\tremaining: 11.5s\n",
            "237:\tlearn: 0.0488590\ttotal: 10.4s\tremaining: 11.5s\n",
            "238:\tlearn: 0.0486423\ttotal: 10.4s\tremaining: 11.4s\n",
            "239:\tlearn: 0.0482979\ttotal: 10.5s\tremaining: 11.3s\n",
            "240:\tlearn: 0.0480812\ttotal: 10.5s\tremaining: 11.3s\n",
            "241:\tlearn: 0.0479668\ttotal: 10.5s\tremaining: 11.2s\n",
            "242:\tlearn: 0.0478324\ttotal: 10.5s\tremaining: 11.1s\n",
            "243:\tlearn: 0.0476023\ttotal: 10.6s\tremaining: 11.1s\n",
            "244:\tlearn: 0.0472652\ttotal: 10.6s\tremaining: 11s\n",
            "245:\tlearn: 0.0470466\ttotal: 10.6s\tremaining: 10.9s\n",
            "246:\tlearn: 0.0468035\ttotal: 10.6s\tremaining: 10.9s\n",
            "247:\tlearn: 0.0465762\ttotal: 10.7s\tremaining: 10.8s\n",
            "248:\tlearn: 0.0464539\ttotal: 10.7s\tremaining: 10.8s\n",
            "249:\tlearn: 0.0462285\ttotal: 10.7s\tremaining: 10.7s\n",
            "250:\tlearn: 0.0460261\ttotal: 10.7s\tremaining: 10.6s\n",
            "251:\tlearn: 0.0458743\ttotal: 10.7s\tremaining: 10.6s\n",
            "252:\tlearn: 0.0457550\ttotal: 10.8s\tremaining: 10.5s\n",
            "253:\tlearn: 0.0456374\ttotal: 10.8s\tremaining: 10.5s\n",
            "254:\tlearn: 0.0453911\ttotal: 10.8s\tremaining: 10.4s\n",
            "255:\tlearn: 0.0452693\ttotal: 10.8s\tremaining: 10.3s\n",
            "256:\tlearn: 0.0450663\ttotal: 10.9s\tremaining: 10.3s\n",
            "257:\tlearn: 0.0449047\ttotal: 10.9s\tremaining: 10.2s\n",
            "258:\tlearn: 0.0447224\ttotal: 10.9s\tremaining: 10.1s\n",
            "259:\tlearn: 0.0444774\ttotal: 10.9s\tremaining: 10.1s\n",
            "260:\tlearn: 0.0443337\ttotal: 10.9s\tremaining: 10s\n",
            "261:\tlearn: 0.0441665\ttotal: 11s\tremaining: 9.96s\n",
            "262:\tlearn: 0.0439466\ttotal: 11s\tremaining: 9.9s\n",
            "263:\tlearn: 0.0437423\ttotal: 11s\tremaining: 9.84s\n",
            "264:\tlearn: 0.0435995\ttotal: 11s\tremaining: 9.78s\n",
            "265:\tlearn: 0.0434311\ttotal: 11.1s\tremaining: 9.72s\n",
            "266:\tlearn: 0.0432475\ttotal: 11.1s\tremaining: 9.66s\n",
            "267:\tlearn: 0.0430935\ttotal: 11.1s\tremaining: 9.6s\n",
            "268:\tlearn: 0.0429455\ttotal: 11.1s\tremaining: 9.55s\n",
            "269:\tlearn: 0.0428224\ttotal: 11.1s\tremaining: 9.49s\n",
            "270:\tlearn: 0.0426578\ttotal: 11.2s\tremaining: 9.43s\n",
            "271:\tlearn: 0.0425787\ttotal: 11.2s\tremaining: 9.37s\n",
            "272:\tlearn: 0.0424648\ttotal: 11.2s\tremaining: 9.31s\n",
            "273:\tlearn: 0.0422883\ttotal: 11.2s\tremaining: 9.26s\n",
            "274:\tlearn: 0.0421178\ttotal: 11.2s\tremaining: 9.2s\n",
            "275:\tlearn: 0.0419767\ttotal: 11.3s\tremaining: 9.14s\n",
            "276:\tlearn: 0.0418799\ttotal: 11.3s\tremaining: 9.09s\n",
            "277:\tlearn: 0.0417633\ttotal: 11.3s\tremaining: 9.03s\n",
            "278:\tlearn: 0.0415775\ttotal: 11.3s\tremaining: 8.97s\n",
            "279:\tlearn: 0.0414248\ttotal: 11.3s\tremaining: 8.92s\n",
            "280:\tlearn: 0.0411653\ttotal: 11.4s\tremaining: 8.86s\n",
            "281:\tlearn: 0.0410487\ttotal: 11.4s\tremaining: 8.81s\n",
            "282:\tlearn: 0.0409452\ttotal: 11.4s\tremaining: 8.75s\n",
            "283:\tlearn: 0.0407964\ttotal: 11.4s\tremaining: 8.7s\n",
            "284:\tlearn: 0.0406818\ttotal: 11.5s\tremaining: 8.65s\n",
            "285:\tlearn: 0.0405841\ttotal: 11.5s\tremaining: 8.59s\n",
            "286:\tlearn: 0.0404833\ttotal: 11.5s\tremaining: 8.54s\n",
            "287:\tlearn: 0.0403378\ttotal: 11.5s\tremaining: 8.48s\n",
            "288:\tlearn: 0.0401527\ttotal: 11.5s\tremaining: 8.43s\n",
            "289:\tlearn: 0.0400301\ttotal: 11.6s\tremaining: 8.38s\n",
            "290:\tlearn: 0.0398735\ttotal: 11.6s\tremaining: 8.32s\n",
            "291:\tlearn: 0.0397292\ttotal: 11.6s\tremaining: 8.27s\n",
            "292:\tlearn: 0.0396088\ttotal: 11.6s\tremaining: 8.22s\n",
            "293:\tlearn: 0.0395234\ttotal: 11.7s\tremaining: 8.18s\n",
            "294:\tlearn: 0.0393784\ttotal: 11.7s\tremaining: 8.12s\n",
            "295:\tlearn: 0.0392360\ttotal: 11.7s\tremaining: 8.07s\n",
            "296:\tlearn: 0.0391276\ttotal: 11.7s\tremaining: 8.02s\n",
            "297:\tlearn: 0.0390057\ttotal: 11.8s\tremaining: 7.97s\n",
            "298:\tlearn: 0.0388939\ttotal: 11.8s\tremaining: 7.92s\n",
            "299:\tlearn: 0.0388120\ttotal: 11.8s\tremaining: 7.87s\n",
            "300:\tlearn: 0.0387017\ttotal: 11.8s\tremaining: 7.82s\n",
            "301:\tlearn: 0.0386324\ttotal: 11.8s\tremaining: 7.76s\n",
            "302:\tlearn: 0.0384723\ttotal: 11.9s\tremaining: 7.71s\n",
            "303:\tlearn: 0.0383499\ttotal: 11.9s\tremaining: 7.66s\n",
            "304:\tlearn: 0.0382152\ttotal: 11.9s\tremaining: 7.61s\n",
            "305:\tlearn: 0.0380862\ttotal: 11.9s\tremaining: 7.56s\n",
            "306:\tlearn: 0.0379492\ttotal: 11.9s\tremaining: 7.51s\n",
            "307:\tlearn: 0.0378140\ttotal: 12s\tremaining: 7.46s\n",
            "308:\tlearn: 0.0376696\ttotal: 12s\tremaining: 7.41s\n",
            "309:\tlearn: 0.0374627\ttotal: 12s\tremaining: 7.36s\n",
            "310:\tlearn: 0.0373474\ttotal: 12s\tremaining: 7.31s\n",
            "311:\tlearn: 0.0372145\ttotal: 12.1s\tremaining: 7.26s\n",
            "312:\tlearn: 0.0370348\ttotal: 12.1s\tremaining: 7.22s\n",
            "313:\tlearn: 0.0369201\ttotal: 12.1s\tremaining: 7.17s\n",
            "314:\tlearn: 0.0367774\ttotal: 12.1s\tremaining: 7.12s\n",
            "315:\tlearn: 0.0366692\ttotal: 12.1s\tremaining: 7.07s\n",
            "316:\tlearn: 0.0366168\ttotal: 12.2s\tremaining: 7.02s\n",
            "317:\tlearn: 0.0365026\ttotal: 12.2s\tremaining: 6.97s\n",
            "318:\tlearn: 0.0364560\ttotal: 12.2s\tremaining: 6.92s\n",
            "319:\tlearn: 0.0363763\ttotal: 12.2s\tremaining: 6.88s\n",
            "320:\tlearn: 0.0361426\ttotal: 12.2s\tremaining: 6.83s\n",
            "321:\tlearn: 0.0360217\ttotal: 12.3s\tremaining: 6.78s\n",
            "322:\tlearn: 0.0359696\ttotal: 12.3s\tremaining: 6.74s\n",
            "323:\tlearn: 0.0358840\ttotal: 12.3s\tremaining: 6.69s\n",
            "324:\tlearn: 0.0357958\ttotal: 12.3s\tremaining: 6.64s\n",
            "325:\tlearn: 0.0357390\ttotal: 12.4s\tremaining: 6.59s\n",
            "326:\tlearn: 0.0356341\ttotal: 12.4s\tremaining: 6.55s\n",
            "327:\tlearn: 0.0355156\ttotal: 12.4s\tremaining: 6.5s\n",
            "328:\tlearn: 0.0353653\ttotal: 12.4s\tremaining: 6.46s\n",
            "329:\tlearn: 0.0352118\ttotal: 12.4s\tremaining: 6.41s\n",
            "330:\tlearn: 0.0350947\ttotal: 12.5s\tremaining: 6.37s\n",
            "331:\tlearn: 0.0349674\ttotal: 12.5s\tremaining: 6.32s\n",
            "332:\tlearn: 0.0348690\ttotal: 12.5s\tremaining: 6.27s\n",
            "333:\tlearn: 0.0348142\ttotal: 12.5s\tremaining: 6.23s\n",
            "334:\tlearn: 0.0346979\ttotal: 12.6s\tremaining: 6.18s\n",
            "335:\tlearn: 0.0346152\ttotal: 12.6s\tremaining: 6.14s\n",
            "336:\tlearn: 0.0345332\ttotal: 12.6s\tremaining: 6.09s\n",
            "337:\tlearn: 0.0344143\ttotal: 12.6s\tremaining: 6.05s\n",
            "338:\tlearn: 0.0343068\ttotal: 12.6s\tremaining: 6s\n",
            "339:\tlearn: 0.0341969\ttotal: 12.7s\tremaining: 5.96s\n",
            "340:\tlearn: 0.0341025\ttotal: 12.7s\tremaining: 5.92s\n",
            "341:\tlearn: 0.0340152\ttotal: 12.7s\tremaining: 5.88s\n",
            "342:\tlearn: 0.0338681\ttotal: 12.7s\tremaining: 5.83s\n",
            "343:\tlearn: 0.0337565\ttotal: 12.8s\tremaining: 5.79s\n",
            "344:\tlearn: 0.0336845\ttotal: 12.8s\tremaining: 5.75s\n",
            "345:\tlearn: 0.0335566\ttotal: 12.8s\tremaining: 5.7s\n",
            "346:\tlearn: 0.0334780\ttotal: 12.8s\tremaining: 5.66s\n",
            "347:\tlearn: 0.0333989\ttotal: 12.9s\tremaining: 5.62s\n",
            "348:\tlearn: 0.0333297\ttotal: 12.9s\tremaining: 5.57s\n",
            "349:\tlearn: 0.0332268\ttotal: 12.9s\tremaining: 5.53s\n",
            "350:\tlearn: 0.0330855\ttotal: 12.9s\tremaining: 5.49s\n",
            "351:\tlearn: 0.0330306\ttotal: 12.9s\tremaining: 5.44s\n",
            "352:\tlearn: 0.0329233\ttotal: 13s\tremaining: 5.4s\n",
            "353:\tlearn: 0.0328081\ttotal: 13s\tremaining: 5.36s\n",
            "354:\tlearn: 0.0326935\ttotal: 13s\tremaining: 5.31s\n",
            "355:\tlearn: 0.0326044\ttotal: 13s\tremaining: 5.27s\n",
            "356:\tlearn: 0.0324587\ttotal: 13s\tremaining: 5.22s\n",
            "357:\tlearn: 0.0323915\ttotal: 13.1s\tremaining: 5.18s\n",
            "358:\tlearn: 0.0323169\ttotal: 13.1s\tremaining: 5.14s\n",
            "359:\tlearn: 0.0321864\ttotal: 13.1s\tremaining: 5.1s\n",
            "360:\tlearn: 0.0319877\ttotal: 13.1s\tremaining: 5.06s\n",
            "361:\tlearn: 0.0318489\ttotal: 13.2s\tremaining: 5.02s\n",
            "362:\tlearn: 0.0317692\ttotal: 13.2s\tremaining: 4.97s\n",
            "363:\tlearn: 0.0317032\ttotal: 13.2s\tremaining: 4.93s\n",
            "364:\tlearn: 0.0315854\ttotal: 13.2s\tremaining: 4.89s\n",
            "365:\tlearn: 0.0315056\ttotal: 13.2s\tremaining: 4.85s\n",
            "366:\tlearn: 0.0313954\ttotal: 13.3s\tremaining: 4.81s\n",
            "367:\tlearn: 0.0312690\ttotal: 13.3s\tremaining: 4.76s\n",
            "368:\tlearn: 0.0312030\ttotal: 13.3s\tremaining: 4.72s\n",
            "369:\tlearn: 0.0311417\ttotal: 13.3s\tremaining: 4.68s\n",
            "370:\tlearn: 0.0310553\ttotal: 13.4s\tremaining: 4.64s\n",
            "371:\tlearn: 0.0309981\ttotal: 13.4s\tremaining: 4.6s\n",
            "372:\tlearn: 0.0308907\ttotal: 13.4s\tremaining: 4.56s\n",
            "373:\tlearn: 0.0308106\ttotal: 13.4s\tremaining: 4.52s\n",
            "374:\tlearn: 0.0307514\ttotal: 13.4s\tremaining: 4.48s\n",
            "375:\tlearn: 0.0306663\ttotal: 13.5s\tremaining: 4.44s\n",
            "376:\tlearn: 0.0305913\ttotal: 13.5s\tremaining: 4.4s\n",
            "377:\tlearn: 0.0305223\ttotal: 13.5s\tremaining: 4.36s\n",
            "378:\tlearn: 0.0304566\ttotal: 13.5s\tremaining: 4.32s\n",
            "379:\tlearn: 0.0303676\ttotal: 13.5s\tremaining: 4.28s\n",
            "380:\tlearn: 0.0302901\ttotal: 13.6s\tremaining: 4.24s\n",
            "381:\tlearn: 0.0301960\ttotal: 13.6s\tremaining: 4.2s\n",
            "382:\tlearn: 0.0301028\ttotal: 13.6s\tremaining: 4.16s\n",
            "383:\tlearn: 0.0300555\ttotal: 13.6s\tremaining: 4.12s\n",
            "384:\tlearn: 0.0300072\ttotal: 13.7s\tremaining: 4.08s\n",
            "385:\tlearn: 0.0299044\ttotal: 13.7s\tremaining: 4.04s\n",
            "386:\tlearn: 0.0297998\ttotal: 13.7s\tremaining: 4s\n",
            "387:\tlearn: 0.0297523\ttotal: 13.7s\tremaining: 3.97s\n",
            "388:\tlearn: 0.0296846\ttotal: 13.8s\tremaining: 3.93s\n",
            "389:\tlearn: 0.0295442\ttotal: 13.8s\tremaining: 3.89s\n",
            "390:\tlearn: 0.0294738\ttotal: 13.8s\tremaining: 3.85s\n",
            "391:\tlearn: 0.0294268\ttotal: 13.8s\tremaining: 3.81s\n",
            "392:\tlearn: 0.0293407\ttotal: 13.8s\tremaining: 3.77s\n",
            "393:\tlearn: 0.0292859\ttotal: 13.9s\tremaining: 3.73s\n",
            "394:\tlearn: 0.0292242\ttotal: 13.9s\tremaining: 3.69s\n",
            "395:\tlearn: 0.0291289\ttotal: 13.9s\tremaining: 3.65s\n",
            "396:\tlearn: 0.0290694\ttotal: 13.9s\tremaining: 3.61s\n",
            "397:\tlearn: 0.0289718\ttotal: 14s\tremaining: 3.58s\n",
            "398:\tlearn: 0.0288694\ttotal: 14s\tremaining: 3.54s\n",
            "399:\tlearn: 0.0288130\ttotal: 14s\tremaining: 3.5s\n",
            "400:\tlearn: 0.0287376\ttotal: 14s\tremaining: 3.46s\n",
            "401:\tlearn: 0.0286426\ttotal: 14s\tremaining: 3.42s\n",
            "402:\tlearn: 0.0285880\ttotal: 14.1s\tremaining: 3.38s\n",
            "403:\tlearn: 0.0285264\ttotal: 14.1s\tremaining: 3.35s\n",
            "404:\tlearn: 0.0284276\ttotal: 14.1s\tremaining: 3.31s\n",
            "405:\tlearn: 0.0283028\ttotal: 14.1s\tremaining: 3.27s\n",
            "406:\tlearn: 0.0282293\ttotal: 14.2s\tremaining: 3.23s\n",
            "407:\tlearn: 0.0281335\ttotal: 14.2s\tremaining: 3.2s\n",
            "408:\tlearn: 0.0280828\ttotal: 14.2s\tremaining: 3.16s\n",
            "409:\tlearn: 0.0280426\ttotal: 14.2s\tremaining: 3.12s\n",
            "410:\tlearn: 0.0279385\ttotal: 14.2s\tremaining: 3.08s\n",
            "411:\tlearn: 0.0278985\ttotal: 14.3s\tremaining: 3.04s\n",
            "412:\tlearn: 0.0278404\ttotal: 14.3s\tremaining: 3.01s\n",
            "413:\tlearn: 0.0277130\ttotal: 14.3s\tremaining: 2.97s\n",
            "414:\tlearn: 0.0276514\ttotal: 14.3s\tremaining: 2.93s\n",
            "415:\tlearn: 0.0275705\ttotal: 14.3s\tremaining: 2.9s\n",
            "416:\tlearn: 0.0275174\ttotal: 14.4s\tremaining: 2.86s\n",
            "417:\tlearn: 0.0274347\ttotal: 14.4s\tremaining: 2.82s\n",
            "418:\tlearn: 0.0273644\ttotal: 14.4s\tremaining: 2.79s\n",
            "419:\tlearn: 0.0273189\ttotal: 14.4s\tremaining: 2.75s\n",
            "420:\tlearn: 0.0272274\ttotal: 14.5s\tremaining: 2.71s\n",
            "421:\tlearn: 0.0271566\ttotal: 14.5s\tremaining: 2.67s\n",
            "422:\tlearn: 0.0271090\ttotal: 14.5s\tremaining: 2.64s\n",
            "423:\tlearn: 0.0270434\ttotal: 14.5s\tremaining: 2.6s\n",
            "424:\tlearn: 0.0269755\ttotal: 14.5s\tremaining: 2.56s\n",
            "425:\tlearn: 0.0269206\ttotal: 14.6s\tremaining: 2.53s\n",
            "426:\tlearn: 0.0268627\ttotal: 14.6s\tremaining: 2.49s\n",
            "427:\tlearn: 0.0267646\ttotal: 14.6s\tremaining: 2.46s\n",
            "428:\tlearn: 0.0267142\ttotal: 14.6s\tremaining: 2.42s\n",
            "429:\tlearn: 0.0266674\ttotal: 14.6s\tremaining: 2.38s\n",
            "430:\tlearn: 0.0266200\ttotal: 14.7s\tremaining: 2.35s\n",
            "431:\tlearn: 0.0265273\ttotal: 14.7s\tremaining: 2.31s\n",
            "432:\tlearn: 0.0264403\ttotal: 14.7s\tremaining: 2.28s\n",
            "433:\tlearn: 0.0263666\ttotal: 14.8s\tremaining: 2.24s\n",
            "434:\tlearn: 0.0263150\ttotal: 14.8s\tremaining: 2.21s\n",
            "435:\tlearn: 0.0262676\ttotal: 14.8s\tremaining: 2.17s\n",
            "436:\tlearn: 0.0262231\ttotal: 14.8s\tremaining: 2.14s\n",
            "437:\tlearn: 0.0261601\ttotal: 14.8s\tremaining: 2.1s\n",
            "438:\tlearn: 0.0261042\ttotal: 14.9s\tremaining: 2.06s\n",
            "439:\tlearn: 0.0260250\ttotal: 14.9s\tremaining: 2.03s\n",
            "440:\tlearn: 0.0259481\ttotal: 14.9s\tremaining: 2s\n",
            "441:\tlearn: 0.0258679\ttotal: 15s\tremaining: 1.96s\n",
            "442:\tlearn: 0.0257570\ttotal: 15s\tremaining: 1.93s\n",
            "443:\tlearn: 0.0256796\ttotal: 15s\tremaining: 1.9s\n",
            "444:\tlearn: 0.0256333\ttotal: 15.1s\tremaining: 1.86s\n",
            "445:\tlearn: 0.0255858\ttotal: 15.1s\tremaining: 1.83s\n",
            "446:\tlearn: 0.0255167\ttotal: 15.1s\tremaining: 1.79s\n",
            "447:\tlearn: 0.0254326\ttotal: 15.2s\tremaining: 1.76s\n",
            "448:\tlearn: 0.0253689\ttotal: 15.2s\tremaining: 1.73s\n",
            "449:\tlearn: 0.0252860\ttotal: 15.3s\tremaining: 1.7s\n",
            "450:\tlearn: 0.0252424\ttotal: 15.3s\tremaining: 1.66s\n",
            "451:\tlearn: 0.0251942\ttotal: 15.3s\tremaining: 1.63s\n",
            "452:\tlearn: 0.0251375\ttotal: 15.4s\tremaining: 1.59s\n",
            "453:\tlearn: 0.0250521\ttotal: 15.4s\tremaining: 1.56s\n",
            "454:\tlearn: 0.0249998\ttotal: 15.4s\tremaining: 1.52s\n",
            "455:\tlearn: 0.0249695\ttotal: 15.5s\tremaining: 1.49s\n",
            "456:\tlearn: 0.0248977\ttotal: 15.5s\tremaining: 1.46s\n",
            "457:\tlearn: 0.0248355\ttotal: 15.5s\tremaining: 1.43s\n",
            "458:\tlearn: 0.0247817\ttotal: 15.6s\tremaining: 1.39s\n",
            "459:\tlearn: 0.0247342\ttotal: 15.6s\tremaining: 1.36s\n",
            "460:\tlearn: 0.0246534\ttotal: 15.7s\tremaining: 1.32s\n",
            "461:\tlearn: 0.0246115\ttotal: 15.7s\tremaining: 1.29s\n",
            "462:\tlearn: 0.0245580\ttotal: 15.8s\tremaining: 1.26s\n",
            "463:\tlearn: 0.0244524\ttotal: 15.8s\tremaining: 1.23s\n",
            "464:\tlearn: 0.0243940\ttotal: 15.8s\tremaining: 1.19s\n",
            "465:\tlearn: 0.0243587\ttotal: 15.9s\tremaining: 1.16s\n",
            "466:\tlearn: 0.0242973\ttotal: 15.9s\tremaining: 1.12s\n",
            "467:\tlearn: 0.0242156\ttotal: 15.9s\tremaining: 1.09s\n",
            "468:\tlearn: 0.0241801\ttotal: 16s\tremaining: 1.05s\n",
            "469:\tlearn: 0.0241263\ttotal: 16s\tremaining: 1.02s\n",
            "470:\tlearn: 0.0240552\ttotal: 16s\tremaining: 988ms\n",
            "471:\tlearn: 0.0239968\ttotal: 16.1s\tremaining: 954ms\n",
            "472:\tlearn: 0.0239484\ttotal: 16.1s\tremaining: 920ms\n",
            "473:\tlearn: 0.0238992\ttotal: 16.2s\tremaining: 887ms\n",
            "474:\tlearn: 0.0238638\ttotal: 16.2s\tremaining: 853ms\n",
            "475:\tlearn: 0.0238291\ttotal: 16.2s\tremaining: 819ms\n",
            "476:\tlearn: 0.0237257\ttotal: 16.3s\tremaining: 785ms\n",
            "477:\tlearn: 0.0236353\ttotal: 16.3s\tremaining: 751ms\n",
            "478:\tlearn: 0.0235878\ttotal: 16.4s\tremaining: 717ms\n",
            "479:\tlearn: 0.0235493\ttotal: 16.4s\tremaining: 683ms\n",
            "480:\tlearn: 0.0234834\ttotal: 16.4s\tremaining: 649ms\n",
            "481:\tlearn: 0.0234377\ttotal: 16.5s\tremaining: 615ms\n",
            "482:\tlearn: 0.0233706\ttotal: 16.5s\tremaining: 581ms\n",
            "483:\tlearn: 0.0233324\ttotal: 16.6s\tremaining: 547ms\n",
            "484:\tlearn: 0.0232911\ttotal: 16.6s\tremaining: 513ms\n",
            "485:\tlearn: 0.0232329\ttotal: 16.6s\tremaining: 479ms\n",
            "486:\tlearn: 0.0231923\ttotal: 16.7s\tremaining: 445ms\n",
            "487:\tlearn: 0.0231481\ttotal: 16.7s\tremaining: 411ms\n",
            "488:\tlearn: 0.0230450\ttotal: 16.7s\tremaining: 376ms\n",
            "489:\tlearn: 0.0230018\ttotal: 16.8s\tremaining: 343ms\n",
            "490:\tlearn: 0.0229643\ttotal: 16.8s\tremaining: 308ms\n",
            "491:\tlearn: 0.0229175\ttotal: 16.9s\tremaining: 274ms\n",
            "492:\tlearn: 0.0228976\ttotal: 16.9s\tremaining: 240ms\n",
            "493:\tlearn: 0.0228664\ttotal: 16.9s\tremaining: 206ms\n",
            "494:\tlearn: 0.0228264\ttotal: 17s\tremaining: 171ms\n",
            "495:\tlearn: 0.0227840\ttotal: 17s\tremaining: 137ms\n",
            "496:\tlearn: 0.0227318\ttotal: 17s\tremaining: 103ms\n",
            "497:\tlearn: 0.0226931\ttotal: 17.1s\tremaining: 68.6ms\n",
            "498:\tlearn: 0.0226441\ttotal: 17.1s\tremaining: 34.3ms\n",
            "499:\tlearn: 0.0225879\ttotal: 17.2s\tremaining: 0us\n",
            "Model Accuracy: 97.27%\n",
            "Enter Nitrogen (N) value: 83\n",
            "Enter Phosphorus (P) value: 85\n",
            "Enter Potassium (K) value: 50\n",
            "Enter temperature value: 26\n",
            "Enter humidity value: 77\n",
            "Enter pH value: 5\n",
            "The recommended crop is: banana\n"
          ]
        }
      ]
    }
  ]
}